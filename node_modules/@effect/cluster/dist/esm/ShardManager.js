/**
 * @since 1.0.0
 */
import * as Rpc from "@effect/rpc/Rpc";
import * as RpcClient from "@effect/rpc/RpcClient";
import * as RpcGroup from "@effect/rpc/RpcGroup";
import * as RpcServer from "@effect/rpc/RpcServer";
import * as Arr from "effect/Array";
import * as Clock from "effect/Clock";
import * as Config_ from "effect/Config";
import * as ConfigProvider from "effect/ConfigProvider";
import * as Context from "effect/Context";
import * as Data from "effect/Data";
import * as Deferred from "effect/Deferred";
import * as Duration from "effect/Duration";
import * as Effect from "effect/Effect";
import * as Equal from "effect/Equal";
import * as FiberSet from "effect/FiberSet";
import { identity } from "effect/Function";
import * as Iterable from "effect/Iterable";
import * as Layer from "effect/Layer";
import * as Mailbox from "effect/Mailbox";
import * as Metric from "effect/Metric";
import * as MetricLabel from "effect/MetricLabel";
import * as MutableHashMap from "effect/MutableHashMap";
import * as MutableHashSet from "effect/MutableHashSet";
import * as Option from "effect/Option";
import * as PubSub from "effect/PubSub";
import * as Queue from "effect/Queue";
import * as Schedule from "effect/Schedule";
import * as Schema from "effect/Schema";
import { RunnerNotRegistered } from "./ClusterError.js";
import * as ClusterMetrics from "./ClusterMetrics.js";
import { addAllNested, decideAssignmentsForShards, State } from "./internal/shardManager.js";
import * as MachineId from "./MachineId.js";
import { Runner } from "./Runner.js";
import { RunnerAddress } from "./RunnerAddress.js";
import { RunnerHealth } from "./RunnerHealth.js";
import { RpcClientProtocol, Runners } from "./Runners.js";
import { make as makeShardId, ShardId } from "./ShardId.js";
import { ShardingConfig } from "./ShardingConfig.js";
import { ShardStorage } from "./ShardStorage.js";
/**
 * @since 1.0.0
 * @category models
 */
export class ShardManager extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager")() {}
/**
 * @since 1.0.0
 * @category Config
 */
export class Config extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager/Config")() {
  /**
   * @since 1.0.0
   */
  static defaults = {
    rebalanceDebounce: /*#__PURE__*/Duration.seconds(3),
    rebalanceInterval: /*#__PURE__*/Duration.seconds(20),
    rebalanceRetryInterval: /*#__PURE__*/Duration.seconds(10),
    rebalanceRate: 2 / 100,
    persistRetryCount: 100,
    persistRetryInterval: /*#__PURE__*/Duration.seconds(3),
    runnerHealthCheckInterval: /*#__PURE__*/Duration.minutes(1),
    runnerPingTimeout: /*#__PURE__*/Duration.seconds(3)
  };
}
/**
 * @since 1.0.0
 * @category Config
 */
export const configConfig = /*#__PURE__*/Config_.all({
  rebalanceDebounce: /*#__PURE__*/Config_.duration("rebalanceDebounce").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceDebounce), /*#__PURE__*/Config_.withDescription("The duration to wait before rebalancing shards after a change.")),
  rebalanceInterval: /*#__PURE__*/Config_.duration("rebalanceInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceInterval), /*#__PURE__*/Config_.withDescription("The interval on which regular rebalancing of shards will occur.")),
  rebalanceRetryInterval: /*#__PURE__*/Config_.duration("rebalanceRetryInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceRetryInterval), /*#__PURE__*/Config_.withDescription("The interval on which rebalancing of shards which failed to be rebalanced will be retried.")),
  rebalanceRate: /*#__PURE__*/Config_.number("rebalanceRate").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.rebalanceRate), /*#__PURE__*/Config_.withDescription("The maximum ratio of shards to rebalance at once.")),
  persistRetryCount: /*#__PURE__*/Config_.integer("persistRetryCount").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.persistRetryCount), /*#__PURE__*/Config_.withDescription("The number of times persistence of runners will be retried if it fails.")),
  persistRetryInterval: /*#__PURE__*/Config_.duration("persistRetryInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.persistRetryInterval), /*#__PURE__*/Config_.withDescription("The interval on which persistence of runners will be retried if it fails.")),
  runnerHealthCheckInterval: /*#__PURE__*/Config_.duration("runnerHealthCheckInterval").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.runnerHealthCheckInterval), /*#__PURE__*/Config_.withDescription("The interval on which runner health will be checked.")),
  runnerPingTimeout: /*#__PURE__*/Config_.duration("runnerPingTimeout").pipe(/*#__PURE__*/Config_.withDefault(Config.defaults.runnerPingTimeout), /*#__PURE__*/Config_.withDescription("The length of time to wait for a runner to respond to a ping."))
});
/**
 * @since 1.0.0
 * @category Config
 */
export const configFromEnv = /*#__PURE__*/configConfig.pipe(/*#__PURE__*/Effect.withConfigProvider(/*#__PURE__*/ConfigProvider.fromEnv().pipe(ConfigProvider.constantCase)));
/**
 * @since 1.0.0
 * @category Config
 */
export const layerConfig = config => Layer.succeed(Config, {
  ...Config.defaults,
  ...config
});
/**
 * @since 1.0.0
 * @category Config
 */
export const layerConfigFromEnv = config => Layer.effect(Config, config ? Effect.map(configFromEnv, env => ({
  ...env,
  ...config
})) : configFromEnv);
/**
 * Represents a client which can be used to communicate with the
 * `ShardManager`.
 *
 * @since 1.0.0
 * @category Client
 */
export class ShardManagerClient extends /*#__PURE__*/Context.Tag("@effect/cluster/ShardManager/ShardManagerClient")() {}
/**
 * @since 1.0.0
 * @category models
 */
export const ShardingEventSchema = /*#__PURE__*/Schema.Union(/*#__PURE__*/Schema.TaggedStruct("StreamStarted", {}), /*#__PURE__*/Schema.TaggedStruct("ShardsAssigned", {
  address: RunnerAddress,
  shards: /*#__PURE__*/Schema.Array(ShardId)
}), /*#__PURE__*/Schema.TaggedStruct("ShardsUnassigned", {
  address: RunnerAddress,
  shards: /*#__PURE__*/Schema.Array(ShardId)
}), /*#__PURE__*/Schema.TaggedStruct("RunnerRegistered", {
  address: RunnerAddress
}), /*#__PURE__*/Schema.TaggedStruct("RunnerUnregistered", {
  address: RunnerAddress
}));
/**
 * The messaging protocol for the `ShardManager`.
 *
 * @since 1.0.0
 * @category Rpcs
 */
export class Rpcs extends /*#__PURE__*/RpcGroup.make(/*#__PURE__*/Rpc.make("Register", {
  payload: {
    runner: Runner
  },
  success: MachineId.MachineId
}), /*#__PURE__*/Rpc.make("Unregister", {
  payload: {
    address: RunnerAddress
  }
}), /*#__PURE__*/Rpc.make("NotifyUnhealthyRunner", {
  payload: {
    address: RunnerAddress
  }
}), /*#__PURE__*/Rpc.make("GetAssignments", {
  success: /*#__PURE__*/Schema.Array(/*#__PURE__*/Schema.Tuple(ShardId, /*#__PURE__*/Schema.Option(RunnerAddress)))
}), /*#__PURE__*/Rpc.make("ShardingEvents", {
  success: ShardingEventSchema,
  stream: true
}), /*#__PURE__*/Rpc.make("GetTime", {
  success: Schema.Number
})) {}
/**
 * @since 1.0.0
 * @category models
 */
export const ShardingEvent = /*#__PURE__*/Data.taggedEnum();
/**
 * @since 1.0.0
 * @category Client
 */
export const makeClientLocal = /*#__PURE__*/Effect.gen(function* () {
  const config = yield* ShardingConfig;
  const clock = yield* Effect.clock;
  const groups = new Set();
  const shards = MutableHashMap.empty();
  let machineId = 0;
  return ShardManagerClient.of({
    register: (_, groupsToAdd) => Effect.sync(() => {
      for (const group of groupsToAdd) {
        if (groups.has(group)) continue;
        groups.add(group);
        for (let n = 1; n <= config.shardsPerGroup; n++) {
          MutableHashMap.set(shards, makeShardId(group, n), config.runnerAddress);
        }
      }
      return MachineId.make(++machineId);
    }),
    unregister: () => Effect.void,
    notifyUnhealthyRunner: () => Effect.void,
    getAssignments: Effect.succeed(shards),
    shardingEvents: Effect.gen(function* () {
      const mailbox = yield* Mailbox.make();
      yield* mailbox.offer(ShardingEvent.StreamStarted());
      return mailbox;
    }),
    getTime: clock.currentTimeMillis
  });
});
/**
 * @since 1.0.0
 * @category Client
 */
export const makeClientRpc = /*#__PURE__*/Effect.gen(function* () {
  const config = yield* ShardingConfig;
  const client = yield* RpcClient.make(Rpcs, {
    spanPrefix: "ShardManagerClient",
    disableTracing: true
  });
  return ShardManagerClient.of({
    register: (address, groups) => client.Register({
      runner: Runner.make({
        address,
        version: config.serverVersion,
        groups
      })
    }),
    unregister: address => client.Unregister({
      address
    }),
    notifyUnhealthyRunner: address => client.NotifyUnhealthyRunner({
      address
    }),
    getAssignments: client.GetAssignments(),
    shardingEvents: client.ShardingEvents(void 0, {
      asMailbox: true
    }),
    getTime: client.GetTime()
  });
});
/**
 * @since 1.0.0
 * @category Client
 */
export const layerClientLocal = /*#__PURE__*/Layer.effect(ShardManagerClient, makeClientLocal);
/**
 * @since 1.0.0
 * @category Client
 */
export const layerClientRpc = /*#__PURE__*/Layer.scoped(ShardManagerClient, makeClientRpc).pipe(/*#__PURE__*/Layer.provide(/*#__PURE__*/Layer.scoped(RpcClient.Protocol, /*#__PURE__*/Effect.gen(function* () {
  const config = yield* ShardingConfig;
  const clientProtocol = yield* RpcClientProtocol;
  return yield* clientProtocol(config.shardManagerAddress);
}))));
/**
 * @since 1.0.0
 * @category Constructors
 */
export const make = /*#__PURE__*/Effect.gen(function* () {
  const storage = yield* ShardStorage;
  const runnersApi = yield* Runners;
  const runnerHealthApi = yield* RunnerHealth;
  const clock = yield* Effect.clock;
  const config = yield* Config;
  const shardingConfig = yield* ShardingConfig;
  const state = yield* Effect.orDie(State.fromStorage(shardingConfig.shardsPerGroup));
  const scope = yield* Effect.scope;
  const events = yield* PubSub.unbounded();
  function updateRunnerMetrics() {
    ClusterMetrics.runners.unsafeUpdate(MutableHashMap.size(state.allRunners), []);
  }
  function updateShardMetrics() {
    const stats = state.shardStats;
    for (const [address, shardCount] of stats.perRunner) {
      ClusterMetrics.assignedShards.unsafeUpdate(shardCount, [MetricLabel.make("address", address)]);
    }
    ClusterMetrics.unassignedShards.unsafeUpdate(stats.unassigned, []);
  }
  updateShardMetrics();
  function withRetry(effect) {
    return effect.pipe(Effect.retry({
      schedule: Schedule.spaced(config.persistRetryCount),
      times: config.persistRetryCount
    }), Effect.ignore);
  }
  const persistRunners = Effect.unsafeMakeSemaphore(1).withPermits(1)(withRetry(Effect.suspend(() => storage.saveRunners(Iterable.map(state.allRunners, ([address, runner]) => [address, runner.runner])))));
  const persistAssignments = Effect.unsafeMakeSemaphore(1).withPermits(1)(withRetry(Effect.suspend(() => storage.saveAssignments(state.assignments))));
  const notifyUnhealthyRunner = Effect.fnUntraced(function* (address) {
    if (!MutableHashMap.has(state.allRunners, address)) return;
    if (!(yield* runnerHealthApi.isAlive(address))) {
      yield* Effect.logWarning(`Runner at address '${address.toString()}' is not alive`);
      yield* unregister(address);
    }
  });
  function updateShardsState(shards, address) {
    return Effect.suspend(() => {
      if (Option.isSome(address) && !MutableHashMap.has(state.allRunners, address.value)) {
        return Effect.fail(new RunnerNotRegistered({
          address: address.value
        }));
      }
      state.addAssignments(shards, address);
      return Effect.void;
    });
  }
  const getAssignments = Effect.sync(() => state.assignments);
  let machineId = 0;
  const register = Effect.fnUntraced(function* (runner) {
    yield* Effect.logInfo(`Registering runner ${Runner.pretty(runner)}`);
    const current = MutableHashMap.get(state.allRunners, runner.address).pipe(Option.filter(r => r.runner.version === runner.version));
    if (Option.isSome(current)) {
      return MachineId.make(++machineId);
    }
    state.addRunner(runner, clock.unsafeCurrentTimeMillis());
    updateRunnerMetrics();
    yield* PubSub.publish(events, ShardingEvent.RunnerRegistered({
      address: runner.address
    }));
    yield* Effect.forkIn(persistRunners, scope);
    yield* Effect.forkIn(rebalance, scope);
    return MachineId.make(++machineId);
  });
  const unregister = Effect.fnUntraced(function* (address) {
    if (!MutableHashMap.has(state.allRunners, address)) return;
    yield* Effect.logInfo("Unregistering runner at address:", address);
    const unassignments = Arr.empty();
    for (const [shard, runner] of state.assignments) {
      if (Option.isSome(runner) && Equal.equals(runner.value, address)) {
        unassignments.push(shard);
      }
    }
    state.addAssignments(unassignments, Option.none());
    state.removeRunner(address);
    updateRunnerMetrics();
    if (unassignments.length > 0) {
      yield* PubSub.publish(events, ShardingEvent.RunnerUnregistered({
        address
      }));
    }
    yield* Effect.forkIn(persistRunners, scope);
    yield* Effect.forkIn(rebalance, scope);
  });
  let rebalancing = false;
  let rebalanceDeferred;
  const rebalanceFibers = yield* FiberSet.make();
  const rebalance = Effect.withFiberRuntime(fiber => {
    if (!rebalancing) {
      rebalancing = true;
      return rebalanceLoop;
    }
    if (!rebalanceDeferred) {
      rebalanceDeferred = Deferred.unsafeMake(fiber.id());
    }
    return Deferred.await(rebalanceDeferred);
  });
  const rebalanceLoop = Effect.suspend(() => {
    const deferred = rebalanceDeferred;
    rebalanceDeferred = undefined;
    return runRebalance.pipe(deferred ? Effect.intoDeferred(deferred) : identity, Effect.onExit(() => {
      if (!rebalanceDeferred) {
        rebalancing = false;
        return Effect.void;
      }
      return Effect.forkIn(rebalanceLoop, scope);
    }));
  });
  const runRebalance = Effect.gen(function* () {
    yield* Effect.sleep(config.rebalanceDebounce);
    if (state.shards.size === 0) {
      yield* Effect.logDebug("No shards to rebalance");
      return;
    }
    // Determine which shards to assign and unassign
    const assignments = MutableHashMap.empty();
    const unassignments = MutableHashMap.empty();
    const changes = MutableHashSet.empty();
    for (const group of state.shards.keys()) {
      const [groupAssignments, groupUnassignments, groupChanges] = decideAssignmentsForShards(state, group);
      for (const [address, shards] of groupAssignments) {
        addAllNested(assignments, address, Array.from(shards, id => makeShardId(group, id)));
      }
      for (const [address, shards] of groupUnassignments) {
        addAllNested(unassignments, address, Array.from(shards, id => makeShardId(group, id)));
      }
      for (const address of groupChanges) {
        MutableHashSet.add(changes, address);
      }
    }
    yield* Effect.logDebug(`Rebalancing shards`);
    if (MutableHashSet.size(changes) === 0) return;
    yield* Metric.increment(ClusterMetrics.rebalances);
    // Ping runners first and remove unhealthy ones
    const failedRunners = MutableHashSet.empty();
    for (const address of changes) {
      yield* FiberSet.run(rebalanceFibers, runnersApi.ping(address).pipe(Effect.timeout(config.runnerPingTimeout), Effect.catchAll(() => {
        MutableHashSet.add(failedRunners, address);
        MutableHashMap.remove(assignments, address);
        MutableHashMap.remove(unassignments, address);
        return Effect.void;
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    const failedUnassignments = new Set();
    for (const [address, shards] of unassignments) {
      yield* FiberSet.run(rebalanceFibers, updateShardsState(shards, Option.none()).pipe(Effect.matchEffect({
        onFailure: () => {
          MutableHashSet.add(failedRunners, address);
          for (const shard of shards) {
            failedUnassignments.add(shard);
          }
          // Remove failed runners from the assignments
          MutableHashMap.remove(assignments, address);
          return Effect.void;
        },
        onSuccess: () => PubSub.publish(events, ShardingEvent.ShardsUnassigned({
          address,
          shards: Array.from(shards)
        }))
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    // Remove failed shard unassignments from the assignments
    MutableHashMap.forEach(assignments, (shards, address) => {
      for (const shard of failedUnassignments) {
        MutableHashSet.remove(shards, shard);
      }
      if (MutableHashSet.size(shards) === 0) {
        MutableHashMap.remove(assignments, address);
      }
    });
    // Perform the assignments
    for (const [address, shards] of assignments) {
      yield* FiberSet.run(rebalanceFibers, updateShardsState(shards, Option.some(address)).pipe(Effect.matchEffect({
        onFailure: () => {
          MutableHashSet.add(failedRunners, address);
          return Effect.void;
        },
        onSuccess: () => PubSub.publish(events, ShardingEvent.ShardsAssigned({
          address,
          shards: Array.from(shards)
        }))
      })));
    }
    yield* FiberSet.awaitEmpty(rebalanceFibers);
    updateShardMetrics();
    const wereFailures = MutableHashSet.size(failedRunners) > 0;
    if (wereFailures) {
      // Check if the failing runners are still reachable
      yield* Effect.forEach(failedRunners, notifyUnhealthyRunner, {
        discard: true
      }).pipe(Effect.forkIn(scope));
      yield* Effect.logWarning("Failed to rebalance runners: ", failedRunners);
    }
    if (wereFailures) {
      // Try rebalancing again later if there were any failures
      yield* Clock.sleep(config.rebalanceRetryInterval).pipe(Effect.zipRight(rebalance), Effect.forkIn(scope));
    }
    yield* persistAssignments;
  }).pipe(Effect.withSpan("ShardManager.rebalance", {
    captureStackTrace: false
  }));
  const checkRunnerHealth = Effect.suspend(() => Effect.forEach(MutableHashMap.keys(state.allRunners), notifyUnhealthyRunner, {
    concurrency: 10,
    discard: true
  }));
  yield* Effect.addFinalizer(() => persistAssignments.pipe(Effect.catchAllCause(cause => Effect.logWarning("Failed to persist assignments on shutdown", cause)), Effect.zipRight(persistRunners.pipe(Effect.catchAllCause(cause => Effect.logWarning("Failed to persist runners on shutdown", cause))))));
  yield* Effect.forkIn(persistRunners, scope);
  // Start a regular cluster rebalance at the configured interval
  yield* rebalance.pipe(Effect.andThen(Effect.sleep(config.rebalanceInterval)), Effect.forever, Effect.forkIn(scope));
  yield* checkRunnerHealth.pipe(Effect.andThen(Effect.sleep(config.runnerHealthCheckInterval)), Effect.forever, Effect.forkIn(scope));
  yield* Effect.gen(function* () {
    const queue = yield* PubSub.subscribe(events);
    while (true) {
      yield* Effect.logInfo("Shard manager event:", yield* Queue.take(queue));
    }
  }).pipe(Effect.forkIn(scope));
  yield* Effect.logInfo("Shard manager initialized");
  return ShardManager.of({
    getAssignments,
    shardingEvents: PubSub.subscribe(events),
    register,
    unregister,
    rebalance,
    notifyUnhealthyRunner,
    checkRunnerHealth
  });
});
/**
 * @since 1.0.0
 * @category layer
 */
export const layer = /*#__PURE__*/Layer.scoped(ShardManager, make);
/**
 * @since 1.0.0
 * @category Server
 */
export const layerServerHandlers = /*#__PURE__*/Rpcs.toLayer(/*#__PURE__*/Effect.gen(function* () {
  const shardManager = yield* ShardManager;
  const clock = yield* Effect.clock;
  return {
    Register: ({
      runner
    }) => shardManager.register(runner),
    Unregister: ({
      address
    }) => shardManager.unregister(address),
    NotifyUnhealthyRunner: ({
      address
    }) => shardManager.notifyUnhealthyRunner(address),
    GetAssignments: () => Effect.map(shardManager.getAssignments, assignments => Array.from(assignments)),
    ShardingEvents: Effect.fnUntraced(function* () {
      const queue = yield* shardManager.shardingEvents;
      const mailbox = yield* Mailbox.make();
      yield* mailbox.offer(ShardingEvent.StreamStarted());
      yield* Queue.takeBetween(queue, 1, Number.MAX_SAFE_INTEGER).pipe(Effect.flatMap(events => mailbox.offerAll(events)), Effect.forever, Effect.forkScoped);
      return mailbox;
    }),
    GetTime: () => clock.currentTimeMillis
  };
}));
/**
 * @since 1.0.0
 * @category Server
 */
export const layerServer = /*#__PURE__*/RpcServer.layer(Rpcs, {
  spanPrefix: "ShardManager",
  disableTracing: true
}).pipe(/*#__PURE__*/Layer.provide(layerServerHandlers));
//# sourceMappingURL=ShardManager.js.map